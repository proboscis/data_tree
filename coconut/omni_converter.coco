from data_tree.coconut.convert import *
from data_tree.coconut.auto_data import AutoSolver,AutoData
from data_tree.coconut.monad import try_monad,Try,Success,Failure
from frozendict import frozendict
from typing import Mapping
from ipywidgets import Text
import ipywidgets as widgets
from itertools import product
from loguru import logger
from collections import namedtuple
import numpy as np

# actually, this kind of pattern matches are too hard to write in pure python. what do I do???
# wait for pep622,pep642, to be implemented in python 3.10

def imagedef2dict(imdef:ImageDef):
    case imdef:
        match ImageDef(data_type,meta):
            case data_type:
                match Numpy(dtype,arrange,ch_rpr,v_range):
                    info = dict(type="numpy",dtype=dtype,arrange=arrange,ch_rpr=ch_rpr,v_range=str(v_range))
                match Torch(dtype,arrange,ch_rpr,v_range):
                    info = dict(type="torch",dtype=dtype,arrange=arrange,ch_rpr=ch_rpr,v_range=str(v_range))
                match PILImages(mode,ch_rpr):
                    info = dict(type="images",ch_rpr=ch_rpr,mode=mode)
                match PILImage(mode,ch_rpr):
                    info = dict(type="image",ch_rpr=ch_rpr,mode=mode)
            else:
                raise RuntimeError(f"cannot convert unknown imagedef:{imdef} to dict.")
            return frozendict(
                **info,
                meta=meta
            )
    else:
        raise RuntimeError(f"cannot convert unknown imdef:{imdef} to dict.")

def cast_imdef_to_dict(state):
    if isinstance(state,ImageDef):
        return [imagedef2dict(state)]

def cast_imdef_str_to_imdef(state):
    if isinstance(state,str):
        try:
            res = str_to_img_def(state)
            return [res]
        except Exception as e:
            pass
def imdef2imdef_str(imdef):
    case imdef:
        match ImageDef(data_type,meta):
            case data_type:
                match Numpy(dtype,arrange,ch_rpr,v_range):
                    base = f"numpy,{dtype},{arrange},{ch_rpr},{v_range}"
                match Torch(dtype,arrange,ch_rpr,v_range):
                    base = f"torch,{dtype},{arrange},{ch_rpr},{v_range}"
                match PILImages(mode,ch_rpr):
                    base = f"images,{mode},{ch_rpr}"
                match PILImage(mode,ch_rpr):
                    base = f"image,{mode},{ch_rpr}"
            else:
                raise RuntimeError(f"cannot convert unknown imagedef:{imdef} to str.")
            if "shape" in meta:
                shape_str = ":".join([str(s) for s in meta["shape"]])
                res =  base+","+shape_str
            else:
                res = base
            #logger.info(f"imdef_to_str:{res}")
            return res
    else:
        raise RuntimeError(f"cannot convert unknown imdef:{imdef} to str.")
def cast_imdef_to_imdef_str(imdef):
    case imdef:
        match ImageDef(_,_):
            res = [imdef2imdef_str(imdef)]
            return res
    else:
        return None

def imgs2tile_shape(shape,w=1024,h=1024,max_image=100,padding=1):
    case shape:
        match (B,H,W):
            ch = 1
        match (B,H,W,ch):
            pass
    #nrow = int(sqrt(len(imgs[:max_image]))+0.5)
    if shape[0] is None:
        # we cannot estimate actual shape.
        return (None,None,ch)
    n_imgs = min(shape[0],max_image)
    nrow = int(sqrt(n_imgs))
    if (nrow*nrow < n_imgs):
        nrow += 1
    r = int((w-((nrow+1)*padding))/nrow) # width/height of each tile
    new_shape = (r*nrow,r*nrow,ch)
    return new_shape

ms_imgs2tile = imgs2tile_shape |> ss_to_ms$

def imgs2tile(imgs,w=1024,h=1024,max_image=100,padding=1):
    mode = imgs[0].mode
    ch = len(mode)
    #nrow = int(sqrt(len(imgs[:max_image]))+0.5)
    n_imgs = len(imgs[:max_image])
    nrow = int(sqrt(n_imgs))
    if (nrow*nrow < n_imgs):
        nrow += 1
    r = int((w-((nrow+1)*padding))/nrow)

    imgs = np.array([(img.resize((r,r)) |> np.array) for img in imgs[:max_image]])
    if ch == 1:
        imgs = imgs[:,:,:,None]
    return make_grid(imgs,nrow,padding=padding)

def rule_imgs2tile(state):
    case state:
        match ImageDef(PILImages(mode,chrpr),meta):
            return [(
                imgs2tile,
                ImageDef(Numpy("uint8","HWC",chrpr,VR_0_255),meta |> ms_imgs2tile),
                "imgs2tile",
                10
            )]


def rule_img2widget(state):
    case state:
        match ImageDef(PILImage(_,_),meta):
            return [(
                infer_widget,
                "widget",
                "infer_widget",
                1
            )]

def dict2imdef(state):
    if isinstance(state,Mapping):
        case state:
            match {"type":"numpy","dtype":_dtype,"arrange":_arng,"ch_rpr":_ch_rpr,"v_range":_v_range,"meta":meta}:
                return [ImageDef(Numpy(_dtype,_arng,_ch_rpr,_v_range),meta)]
            match {"type":"torch","dtype":_dtype,"arrange":_arng,"ch_rpr":_ch_rpr,"v_range":_v_range,"meta":meta}:
                return [ImageDef(Torch(_dtype,_arng,_ch_rpr,_v_range),meta)]
            match {"type":"image","mode":_mode,"ch_rpr":_ch_rpr,"meta":meta}:
                return [ImageDef(PILImage(_mode,_ch_rpr),meta)]
            match {"type":"images","mode":_mode,"ch_rpr":_ch_rpr,"meta":meta}:
                return [ImageDef(PILImages(_mode,_ch_rpr),meta)]

def rule_numpy2img(state):
    if isinstance(state,Mapping):
        case state:
            match {"type":"numpy","dtype":"uint8","ch_rpr":"RGB","arrange":"HWC","v_range":"0_255","meta":meta}:
                return [(
                    Image.fromarray,
                    ImageDef(PILImage("RGB","RGB"),meta),
                    "Image.fromarray",
                    1
                )]
            match {"type":"numpy","dtype":"uint8","ch_rpr":"L","arrange":"HW","v_range":"0_255","meta":meta}:
                return [(
                    Image.fromarray,
                    ImageDef(PILImage("L","L"),meta),
                    "Image.fromarray",
                    1
                )]

ms_img2L = (s->(s[0],s[1])) |> ss_to_ms$
ms_img2LA = (s->(s[0],s[1],2)) |> ss_to_ms$
def rule_image2gray(state):
    case state:
        match ImageDef(PILImage(ch_rpr,ch_rpr2),meta):
            return [
                (.convert("L"),ImageDef(PILImage("L","L"),meta|>ms_img2L),"image2gray",10),
                (.convert("LA"),ImageDef(PILImage("LA","LA"),meta|>ms_img2LA),"image2gray-alpha",10),
                   ]

def rule_image2lab(state):
    from skimage import color
    case state:
        match ImageDef(Numpy("float64","HWC","RGB","0_1"),meta):
            return [
                (color.rgb2lab,ImageDef(Numpy("float64","HWC","LAB","LAB"),meta),"sklearn.color.rgb2lab")
            ]
        match ImageDef(Numpy("float64","HWC","LAB","LAB"),meta):
            return [
                (color.lab2rgb,ImageDef(Numpy("float64","HWC","RGB","0_1"),meta),"sklearn.color.lab2rgb")
            ]

def convert_ignore_channel(ary,f):
    """
    shape:(H,W,C)
    """
    ignored = ary[:,:,[-1]]
    tgt = ary[:,:,:-1]
    converted = f(tgt)
    result = np.concatenate((tgt,ignored),axis=-1)
    return result

def rule_rgba2laba(state):
    from skimage import color
    case state:
        match ImageDef(Numpy("float64","HWC","RGBA","0_1"),meta):
            return [
                (a->convert_ignore_channel(a,color.rgb2lab),ImageDef(Numpy("float64","HWC","LABA","LABA"),meta),"rgba2laba (ignores alpha)")
            ]
        match ImageDef(Numpy("float64","HWC","LABA","LABA"),meta):
            return [
                (a->convert_ignore_channel(a,color.lab2rgb),ImageDef(Numpy("float64","HWC","RGBA","0_1"),meta),"laba2rgba (ignores alpha)")
            ]


def rule_lab_value_conversion(state):
    case state:
        match ImageDef(Numpy("float64","HWC","LAB","LAB"),meta):
            return [((_vr_lab_to_0_1,ImageDef(Numpy("float64","HWC","LAB","0_1"),meta),"vr_lab_to_0_1"))]
        match ImageDef(Numpy("float64","HWC","LABA","LABA"),meta):
            return [((a->convert_ignore_channel(a,_vr_lab_to_0_1),ImageDef(Numpy("float64","HWC","LABA","0_1"),meta),"vr_laba_to_0_1"))]
        match ImageDef(Numpy("float64","HWC","LAB","0_1"),meta):
            return [((_0_1_to_vr_lab,ImageDef(Numpy("float64","HWC","LAB","LAB"),meta),"0_1_to_vr_lab"))]
        match ImageDef(Numpy("float64","HWC","LABA","0_1"),meta):
            return [((a->convert_ignore_channel(a,_0_1_to_vr_lab),ImageDef(Numpy("float64","HWC","LABA","LABA"),meta),"vr_0_1_to_laba"))]

def _vr_lab_to_0_1(ary):
    r = ary.copy()
    r[:,:,0] = ary[:,:,0] * 0.01
    r[:,:,1] = (ary[:,:,1] + 128.0) / 255.0
    r[:,:,2] = (ary[:,:,2] + 128.0) / 255.0
    return r

def _0_1_to_vr_lab(ary):
    r = ary.copy()
    r[:,:,0] = ary[:,:,0] * 100
    r[:,:,1] = (ary[:,:,1] * 255) - 128.0
    r[:,:,2] = (ary[:,:,2] * 255) - 128.0
    return r

def any2widget(state):
    return [(ary->Text(str(ary)),"widget","anything_to_text_widget",1000)]
data AutoTuple(formats is tuple)

def auto_tuple2widget(state):
    case state:
        match AutoTuple(items) if all(i == "widget" for i in items):
            return [
                (
                values-> widgets.VBox(values),
                "widget",
                "auto_tuple of widgets to a widget",
                1
                )
            ]

def isnamedtuple(x):
    t = type(x)
    b = t.__bases__
    if hasattr(x,"__slots__"): return True
    if len(b) != 1 or b[0] != tuple: return False
    f = getattr(t, '_fields', None)
    if not isinstance(f, tuple): return False
    return all(type(n)==str for n in f)

def cast_tuple2auto_tuple(state):
    if isinstance(state,str):
        return None
    if isinstance(state,AutoTuple):
        res = [state.formats]
        return res
    elif type(state) == tuple:
        res = [AutoTuple(state)]
        return res

def map_tuple_i(t,i,f):
    res = list(t)
    res[i] = f(res[i])
    return tuple(res)

def map_state(states,i,new_state):
    res = list(states)
    res[i] = new_state
    return tuple(res)

def intra_tuple_conversions(state):
    case state:
        match AutoTuple(items):
            # I think I should only change a part of state.
            res = []
            for i in range(len(items)):
                ith_state = items[i]
                res += [
                ((f,new_state,cost,name)->(
                    values->map_tuple_i(values,i,f),
                    AutoTuple(map_state(items,i,new_state)),
                    f"map {i}th element with {name}",
                    cost
                ))(f,new_state,cost,name) for f,new_state,cost,name in SOLVER.solver.neighbors(ith_state)]
            return res


def map_each(t,mappers):
    #logger.warning(f"items:{t}")
    #logger.warning(f"mappers:{mappers}")
    return tuple([f(item) for f,item in zip(mappers,t)])

def smart_tuple_conversion(state,end):
    case (state,end):
        match (AutoTuple(formats),t) if type(t) == tuple and len(formats) == len(t):
            cs = []
            cost = 0
            for i in range(len(formats)):
                c = SOLVER.solver.search_direct(formats[i],end[i])
                cost += sum(e.cost for e in c.edges)
                cs.append(c)
            res = [
                (t->map_each(t,cs),
                end,
                f"{state}->{end}",
                cost)
            ]
            logger.debug(res)
            return res
        match (AutoTuple(formats),"widget"):
            f,new_state,name,cost = smart_tuple_conversion(state,("widget",)*len(state.formats))[0]
            logger.debug(f"cost:{cost}")
            return [(
                t-> widgets.VBox(f(t)),
                end,
                f"{state}->{end}",
                cost+1
            )]

data AutoList(state):
    def __repr__(self):
        return f"[{self.state}]"


def unlist(items):
    return SOLVER.new_auto_data([i.value for i in items],AutoList(items[0].format))

def cast_ary_str_to_ary_type(state):
    case state:
        match "[" + element_state + "]":
            return [AutoList(element_state)]
        match AutoList(es is str):
            return [f"[{es}]"]

def intra_list_conversions(state):
    case state:
        match AutoList(es):
            return [((f,new_state,cost,name)->(
                items -> [f(i) for i in items],
                AutoList(new_state),
                f"[{name}]",
                cost+1
            ))(f,new_state,cost,name) for f,new_state,cost,name in SOLVER.solver.neighbors(es)]

#shape change!
ms_add_None_b_ch = (s->(None,*s)) |> ss_to_ms$
"adds 1 as batch shape"
ms_del_b_ch = (s->s[1:])  |> ss_to_ms$
# TODO preserve length in AutoList
def img_list_is_imgs(state):
    case state:
        match AutoList(ImageDef(PILImage(m1,m2),meta)):
            return [ImageDef(PILImages(m1,m2),meta |> ms_add_None_b_ch)]
        match ImageDef(PILImages(m1,m2),meta):
            return [AutoList(ImageDef(PILImage(m1,m2),meta |> ms_del_b_ch))]
def numpys_to_numpy(state):
    case state:
        match AutoList({"type":"numpy","arrange":arng,"meta":{"shape":shape,**other_meta},**kwargs}) if "B" not in arng:
            return [
                (numpys->np.array(numpys),
                frozendict({"type":"numpy","arrange":"B"+arng,"meta":fdict({"shape":(None,)+shape,**other_meta}),**kwargs}),
                f"merge arrays to array",
                10)
            ]
def tensor_to_list(state):
    case state:
        match {"arrange":"B"+arng,"meta":{"shape":shape,**other_meta},**kwargs} if len(arng) > 1:
            return [
                (tensor->[t for t in tensor],
                AutoList(fdict(arrange=arng,meta=fdict({"shape":shape[1:],**other_meta}),**kwargs)),
                f"tensor to list of tensor",
                2)
            ]
        match {"arrange":"C"+arng,"meta":{"shape":shape,**other_meta},"ch_rpr":cr,**kwargs} if len(arng) > 1:
            return [
                (tensor->[t for t in tensor],
                AutoList(fdict(
                    arrange=arng,
                    meta=fdict({"shape":shape[1:],**other_meta}),
                    ch_rpr="L",**kwargs)),
                f"split CHW to [HW]",
                2)
            ]

def pil_convert(state):
    case state:
        match {"type":"image",**kwargs}:
            new_state = dict(**state)
            return [
                (img -> mode -> SOLVER.new_auto_data(img.convert(mode),f"image,{mode},{mode}"),
                "pil_convert",
                "image_to_pil_converter",
                1)
            ]

def rgb_to_rgba(state):
    if state == "numpy,uint8,HWC,RGB,0_255":
        return [(
            a->np.concatenate((a,np.ones((*a.shape[:2],1),dtype="uint8")*255),axis=2),
            "numpy,uint8,HWC,RGBA,0_255",
            "add 255 as alpha channel",
            10
        )]
    elif state == "numpy,uint8,BHWC,RGB,0_255":
        return [(
            a->np.concatenate((a,np.ones((*a.shape[:3],1),dtype="uint8")*255),axis=3),
            "numpy,uint8,BHWC,RGBA,0_255",
            "add 255 as alpha channel to batch",
            10
        )]

@memoize()
def pix2pix_normalizer(nc):
    import torchvision.transforms as transforms
    return transforms.Normalize((0.5,)*nc,(0.5,)*nc)


def torch_img_to_pixpix_input(state):
    import torch
    case state:
        match {"type":"torch","dtype":"float32","arrange":"CHW","v_range":"0_1","ch_rpr":("RGB" or "RGBA" or "L") as rpr,**kwargs}:
            return [(
                pix2pix_normalizer(len(rpr)),
                f"pix2pix,nc={len(rpr)}",
                "to pixpix normalized",
                1
            )]
        match {"type":"torch","dtype":"float32","arrange":"BCHW","v_range":"0_1","ch_rpr":("RGB" or "RGBA" or "L") as rpr,**kwargs}:
            return [(
                t->torch.cat([pix2pix_normalizer(len(rpr))(i)[None] for i in t],dim=0),
                f"pix2pix_batch,nc={len(rpr)}",
                "to pixpix normalized",
                1
            )]
        match "pix2pix_laba":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,CHW,LABA,0_1",
                "inverse pix2pix_laba to img ",
                1
            )]
        match "pix2pix_lab":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,CHW,LAB,0_1",
                "inverse pix2pix_lab to img ",
                1
            )]
        match "pix2pix_laba_batch":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,BCHW,LABA,0_1",
                "inverse pix2pix_laba batch to img",
                1
            )]
        match "pix2pix_lab_batch":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,BCHW,LAB,0_1",
                "inverse pix2pix_laba batch to img",
                1
            )]
        match "pix2pix,nc=4":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,CHW,RGBA,0_1",
                "inverse pix2pix to img",
                1
            )]
        match "pix2pix_batch,nc=4":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,BCHW,RGBA,0_1",
                "inverse pix2pix batch nc=4 to img",
                1
            )]
        match "pix2pix_batch,nc=3":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,BCHW,RGB,0_1",
                "inverse pix2pix batch nc=3 to img",
                1
            )]
        match "pix2pix,nc=3":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,CHW,RGB,0_1",
                "inverse pix2pix to img",
                1
            )]
        match "pix2pix_batch,nc=1":
            return [(
               a -> a*0.5+0.5,
               f"torch,float32,BCHW,L,0_1",
               "inverse pix2pix_batch,nc=1 to img",
               1
            )]
        match "pix2pix,nc=1":
            return [(
                a -> a*0.5+0.5,
                f"torch,float32,CHW,L,0_1",
                "inverse pix2pix,nc=1 to img",
                1
                )]

@memoize()
def _VGG_NORMALIZER():
    import torchvision.transforms as transforms
    nrm = transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])
    return nrm
def inverse_vgg_prep(tensor):
    return tensor * torch.tensor([0.229,0.224,0.225])[:,None,None] + torch.tensor([0.485,0.456,0.406])[:,None,None]
def inverse_vgg_prep_batch(tensor):
    return tensor * torch.tensor([0.229,0.224,0.225])[None,:,None,None] + torch.tensor([0.485,0.456,0.406])[None,:,None,None]
def torch_img_to_vgg_prep(state):
    VGG_NORMALIZER = _VGG_NORMALIZER()
    case state:
        match "vgg_prep":
            return [(
                inverse_vgg_prep,
                "torch,float32,CHW,RGB,0_1",
                "inverse from vgg_prep",
                1)
            ]
        match "vgg_prep_batch":
            return [(
                inverse_vgg_prep_batch,
                "torch,float32,BCHW,RGB,0_1",
                "inverse from vgg_prep_batch",
                1)
            ]
        match {"type":"torch","dtype":"float32","arrange":"CHW","v_range":"0_1","ch_rpr":"RGB",**kwargs}:
            return [(
                VGG_NORMALIZER,
                f"vgg_prep",
                "to vgg normalized",
                1
            )]
        match {"type":"torch","dtype":"float32","arrange":"BCHW","v_range":"0_1","ch_rpr":"RGB",**kwargs}:
            return [(
                t->torch.cat([VGG_NORMALIZER(i)[None] for i in t],dim=0),
                f"vgg_prep_batch",
                "to vgg normalized batch",
                1
            )]
        match {"type":"torch","dtype":"float32","arrange":"BCHW","v_range":"0_1","ch_rpr":"RGBA",**kwargs}:
            return [(
                t->torch.cat([
                    torch.cat((VGG_NORMALIZER(i[:3])[None],i[[3]][None]),dim=1) for i in t
                ],dim=0),
                f"vgg_prep_batch_masked",
                "to vgg normalized batch",
                1
            )]
def repeat_ch(state):
    case state:
        match {"type":"image","mode":"L","ch_rpr":ch,**kwargs} if len(ch) == 1:
            return [
                (a->np.repeat(np.array(a)[:,:,None],3,axis=2),
                 frozendict(type="numpy",dtype="uint8",arrange="HWC",ch_rpr=ch*3,v_range="0_255"),
                "repeat_channel_3",
                50)
            ]



def lll_is_rgb(state):
    case state:
        match {"ch_rpr":"LLL",**kwargs}:
            return [frozendict(ch_rpr="RGB",**kwargs)]



DEFAULT_RULES = AutoImage.default_rules.copy() + [
    AutoSolver.create_cast_rule(cast_imdef_to_dict,"cast_imdef_to_dict"),
    AutoSolver.create_cast_rule(cast_imdef_str_to_imdef,"cast_imdef_str_to_imdef"),
    AutoSolver.create_cast_rule(cast_imdef_to_imdef_str,"cast_imdef_to_imdef_str"),
    AutoSolver.create_cast_rule(dict2imdef,"dict2imdef"),
    AutoSolver.create_cast_rule(cast_ary_str_to_ary_type,"cast_ary_str_to_ary_type"),
    AutoSolver.create_cast_rule(img_list_is_imgs,"img_list_is_imgs"),
    AutoSolver.create_cast_rule(lll_is_rgb,"lll_is_rgb",cost=10),
    AutoSolver.create_cast_rule(cast_tuple2auto_tuple,"tuple <--> auto_tuple"),
    #AutoSolver.create_cast_rule(dict2visdomable),
    AutoSolver.create_conversion_rule(any2widget),
    AutoSolver.create_conversion_rule(rule_imgs2tile),
    AutoSolver.create_conversion_rule(rule_img2widget),
    AutoSolver.create_conversion_rule(rule_numpy2img),
    AutoSolver.create_conversion_rule(rule_image2gray),
    AutoSolver.create_conversion_rule(rule_image2lab),
    AutoSolver.create_conversion_rule(rule_rgba2laba),
    AutoSolver.create_conversion_rule(rule_lab_value_conversion),
    AutoSolver.create_conversion_rule(intra_list_conversions),
    AutoSolver.create_conversion_rule(numpys_to_numpy),
    AutoSolver.create_conversion_rule(tensor_to_list),
    AutoSolver.create_conversion_rule(pil_convert),
    AutoSolver.create_conversion_rule(rgb_to_rgba),
    AutoSolver.create_conversion_rule(repeat_ch),
    AutoSolver.create_conversion_rule(torch_img_to_pixpix_input),
    AutoSolver.create_conversion_rule(torch_img_to_vgg_prep),
    #AutoSolver.create_conversion_rule(intra_tuple_conversions),

    AutoSolver.create_conversion_rule(auto_tuple2widget),
    AutoSolver.create_alias_rule("numpy_rgb","numpy,uint8,HWC,RGB,0_255"),
    AutoSolver.create_alias_rule("numpy_rgba","numpy,uint8,HWC,RGBA,0_255"),
]

SMART_RULES =[
    AutoSolver.create_smart_conversion_rule(smart_tuple_conversion),
]

try:
    import wandb
    def img_to_wandb_img(state):
        case state:
            match ImageDef(PILImage(_,_),_):
                return [(
                    img->wandb.Image(img),
                    "wandb.Image",
                    "image to wandb image",
                    1
                )]
    DEFAULT_RULES.append(AutoSolver.create_conversion_rule(img_to_wandb_img))
    logger.warning(f"added wandb related conversions")
except Exception as e:
    logger.warning(f"could not add wandb related conversions since wandb could not be imported")


def tuple_distance(x,y):
    assert len(x) == len(y),"cannot compare two tuples with different length"
    return len(x) - sum(tuple([i==j for i,j in zip(x,y)]))


@memoize()
def state_distance(x,y):
    conversion = SOLVER.solver.search_direct(x,y,silent=True)
    d = len(conversion.edges)
    #logger.info(f"heuristic conversion:{conversion}")
    #logger.info(f"{x} to {y}:{d}")
    return d

@memoize()
def tuple_state_distance(x,y):
    return sum([state_distance(i,j) for i,j in  zip(x,y)])

@memoize()
def tuple_widget_heuristics(x,y):
    """
    you have to make the solver solve one by one.
    """
    res = 0
    return 0
    if type(x) == tuple and type(y) == tuple:
        if len(x) == len(y):
            res = tuple_distance(x,y)
    if isinstance(x,AutoTuple) and type(y) == tuple:
        if len(x.formats) == len(y):
            xs = x.formats
            ys = y
            res = tuple_distance(xs,ys)
    elif isinstance(x,AutoTuple) and y == "widget":
        xs = x.formats
        ys = ("widget",)*len(x.formats)
        res = tuple_distance(xs,ys)
    #if res == 0:
    #    logger.info(f"{x}->{y}:{res}")
    #    pass
    return res

def tuple_edge_cutter(x,y,end):
    return False
    if isinstance(x,AutoTuple) and type(y) == tuple and type(end) == tuple:
         n = len(x.formats)
         if n == len(y) and n == len(end):
             x2end = tuple_distance(x.formats,end)
             y2end = tuple_distance(y,end)
             x_matching = n - x2end
             y_matching = n - y2end
             if y_matching < x_matching:
                 logger.debug(f"cut {x} to {y} for {end}")
                 return True
    return False


def debug(conv):
    n = conv.edges[-1]
    case (n.src,n.dst):
        match (_,ImageDef(TensorLike(_,arng,*_),{"shape":s,**_})):
            if len(arng) != len(s):
                from IPython import embed
                logger.warning(f"debug console for omni_converter")
                embed()
                raise RuntimeError("exit")
        match (ImageDef(TensorLike(_,arng1,*_),{"shape":s1,**_}),ImageDef(TensorLike(_,arng2,*_),{"shape":s2})):
            if (len(arng2) != len(s2)) or (len(arng1) != len(s1)):
                from IPython import embed
                logger.warning(f"debug console for omni_converter")
                embed()
                raise RuntimeError("exit")


SOLVER = AutoSolver(
    rules=DEFAULT_RULES.copy(),
    smart_rules=SMART_RULES.copy(),
    heuristics=tuple_widget_heuristics,
    edge_cutter=tuple_edge_cutter,
    debug_hook=debug
    )
auto_img = format->value->AutoData(value,format,SOLVER)
