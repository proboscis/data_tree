from PIL import Image
import numpy as np
import heapq
from data_tree.coconut.visualization import infer_widget
from loguru import logger
from data_tree.coconut.astar import new_conversion,AStarSolver,NoRouteException
from math import sqrt
from PIL import Image
import torch
import re
from frozendict import frozendict as fdict
from frozendict import frozendict
VR_0_1 = "0_1"
VR_0_255 = "0_255"
VR_None = "None"
VR_XYZ_Normalized = "XYZ_Normalized"
ch_splitter = re.compile("[A-Z][a-z]*").findall

data DataType
#TODO add shape information to tensorlike
#TODO add shape information to PILImage
#TODO make rules be able to handle other metadata.
#so let's assume the input state based on dict.
data TensorLike(
    dtype is str,
    arrange is str,
    channel_repr is str,
    value_range is str) from DataType:
    def __repr__(self):
        return f"Tensor({self.data_type}|{self.dtype}|{self.arrange}|{self.channel_repr}|{self.value_range})"

data Numpy(dtype,arrange,channel_repr,value_range) from TensorLike:
    def __new__(cls,*args):
        return makedata(cls,*args)
    def __repr__(self):
        return f"Numpy({self.dtype},{self.arrange},{self.channel_repr},{self.value_range})"

data Torch(dtype,arrange,channel_repr,value_range) from TensorLike:
    def __new__(cls,*args):
        return makedata(cls,*args)
    def __repr__(self):
        return f"Torch({self.dtype},{self.arrange},{self.channel_repr},{self.value_range})"

data Hdf5(dtype,arrange,channel_repr,value_range) from TensorLike:
    def __new__(cls,*args):
        return makedata(cls,*args)
    def __repr__(self):
        return f"Hdf5({self.dtype},{self.arrange},{self.channel_repr},{self.value_range})"

data PILImages(mode,channel_repr) from DataType: # represents iterable of PIL.Images
    def __repr__(self):
        return f"PILImages({self.mode},{self.channel_repr})"
data PILImage(mode,channel_repr) from DataType:
    def __repr__(self):
        return f"PILImage({self.mode},{self.channel_repr})"

data ImageDef(data_type is DataType,meta is frozendict):
    def __repr__(self):
        if self.meta:
            return f"ImageDef({self.data_type}|{self.meta})"
        else:
            return f"ImageDef({self.data_type})"

DTYPES={"float32","float64","int32","int64","uint8","bool"}
data DataEdge(a is DataType,b is DataType,f,cost is int,name is str,meta_shifter=x->x):
    def to_edge(self,src:ImageDef):
        try:
            new_meta = self.meta_shifter(src.meta)
            return Edge(src,ImageDef(self.b,new_meta),self.f,self.cost,self.name)
            case (self.a,src.meta,self.b,new_meta):
                match (TensorLike(_,arng1,*_),{"shape":s1},TensorLike(_,arng2,*_),{"shape":s2}):
                    assert len(arng2) == len(s2)
                    assert len(arng1) == len(s1)
            if src.meta != new_meta:
                if "shape" in src.meta and "shape" in new_meta:
                    if len(src.meta["shape"]) > len(new_meta["shape"]):
                        logger.info(f"{self.name}:\n{src.meta['shape']}->{new_meta['shape']}")
        except Exception as e:
            from IPython import embed
            embed()
            raise e
data Edge(a is ImageDef,b is ImageDef,f,cost is int,name="undefined"):
    def __repr__(self):
        return f"{self.a} \t-> {self.name}\t-> {self.b}"
from typing import List

#純粋にエッジだけを考えると、どうしても組み合わせ爆発になる。目的地を知っていれば削減できる。

# 各imdefのNodeベースでエッジを定義するのか。それとも。エッジのルールを網羅するのか。
# オペレーターのほうが数が少ないので、オペレーターだけ定義したい。
# List up operators.
# operator definition: ImageDef->List[DataEdge]
# 1. change data_type
# 2. change dtype
# 3. change arrange
# 4. change ch_repr
# 5. select channel
def to_imagedef(f):
    def _inner(imdef:ImageDef):
        try:
            #logger.debug(type(imdef))
            if imdef `isinstance` ImageDef and len(imdef) >= 1 and imdef `hasattr` "data_type":
            #if imdef `isinstance` ImageDef:
                edges = f(imdef.data_type)
                if edges is not None:
                    return [e.to_edge(imdef) for e in edges]
                else:
                    return []
            else:
                return []
        except Exception as e:
            logger.warning(f"unknown error...imdef:{imdef}")
            logger.warning(f"{imdef} has attr causes exception?")
            logger.warning(f"{hasattr(imdef,'data_type')}")
            raise e
    return _inner

# there are kinds of rules
# 1. doesnt care about shape, and no shape change
# 2. doesnt care shape, but shape changes
# 3. depends on input shape but shape doesn't change
# 4. depends on input and output shape changes
# well ,just give function a shape and return nop for shape?
# what if some other metadata is added?
# where should I store shape?
# If I change the signature, I have modify them all... for match syntax
# I just


@to_imagedef # doesn't change shape, so no touch
def to_PILImages(imdef:ImageDef)->Edge[]:
    case imdef:
        match Numpy("uint8","BHWC", c_repr, =VR_0_255) if len(ch_splitter(c_repr)) in (3,4):
            return [DataEdge(imdef,PILImages(c_repr,c_repr),ary -> [(Image.fromarray)(img) for img in ary],2,name=f"numpy batch {c_repr} to Images")]
        match Numpy("uint8","BHW",c_repr,=VR_0_255):
            return [DataEdge(imdef,PILImages("L",c_repr),ary -> [(Image.fromarray ..> .convert("L"))(img) for img in ary],2,name="numpy batch to images")]
        match Numpy("uint8","HW",c_repr,=VR_0_255):
            return [DataEdge(imdef,PILImage("L",c_repr), Image.fromarray ..> .convert("L"),2,name="numpy HW to PIL Image")]
    return []
@to_imagedef
def to_numpy(imdef:ImageDef)->List[DataEdge]:
    case imdef:
        match Torch(dtype,arng,ch_repr,vr): # torch tensor can always become numpy
            return [DataEdge(imdef,
                         Numpy(dtype  ,arng ,ch_repr,vr),
                         (.detach() ..> .cpu() ..> .numpy()),
                         1,
                         name="torch_to_numpy")]
        match PILImage("RGB",ch_repr):
                    return [DataEdge(imdef,
                                Numpy("uint8","HWC",ch_repr,VR_0_255),
                                np.array,
                                1,
                                name="image_to_numpy")]
        match PILImage("L",ch_repr):
                    return [DataEdge(imdef,
                                Numpy("uint8","HW",ch_repr,VR_0_255),
                                np.array,
                                1,
                                name="image_to_numpy")]
        match PILImage("YCbCr",ch_repr):
                    return [DataEdge(imdef,
                                 Numpy("uint8","HWC",ch_repr,VR_0_255),
                                 np.array,
                                 1,
                                 name="YCbCr image to numpy"
                    )]

        match PILImages("L",ch_repr): # A grayscale Image becomes a numpy array
            return [DataEdge(imdef,
                         Numpy("uint8","BHW",ch_repr,VR_0_255),
                         (fmap$(np.array) ..> np.array),
                         1,
                         name="image_to_numpy")]
        match PILImages(mode,ch_repr):# A multi-channel Image becomes a numpy array
            return [DataEdge(imdef,
                         Numpy("uint8","BHWC",ch_repr,VR_0_255),
                         (fmap$(np.array) ..> np.array),
                         1,
                         name="image_to_numpy")]
    return []
@to_imagedef
def to_torch(imdef):
    import torch
    case imdef:
        match Numpy(dtype,arng,ch_repr,vr): # only numpy can directly become a torch tensor
            return [DataEdge(imdef,Torch(dtype,arng,ch_repr,vr),torch.from_numpy,2,name="to_torch")]
    return []
@to_imagedef
def change_dtype(imdef:ImageDef):# TODO match value range to dtype with bool type
    case imdef:
        match Numpy(dtype,arng,ch_repr,vr):
            return [DataEdge(
                imdef,
                imdef.__class__(_dtype,arng,ch_repr,vr),
                .astype(_dtype),
                1,
                name=f"{dtype} to {_dtype}"
            ) for _dtype in DTYPES if _dtype != dtype]
    return []

# SHAPE SHIFTING RULES


def ss_to_ms(ss,meta):
    if "shape" in meta:
        shape=meta["shape"]
        new_shape = ss(shape)
        return fdict({**meta,"shape":new_shape})
    return meta

ms_0231 = (s->(s[0],s[2],s[3],s[1])) |> ss_to_ms$
ms_0312 = (s->(s[0],s[3],s[1],s[2])) |> ss_to_ms$

def change_arng(imdef):
    case imdef:
        match Numpy(_,"BCHW",_,_):
            return [(.transpose(0,2,3,1),"BHWC",ms_0231)]
        match Numpy(_,"BHWC",_,_):
            return [(.transpose(0,3,1,2),"BCHW",ms_0312)]
        match Torch(_,"BCHW",_,_):
            return [(.transpose(1,2) ..> .transpose(2,3),"BHWC",ms_0231)]
        match Torch(_,"BHWC",_,_):
            return [(.transpose(2,3) ..> .transpose(1,2),"BCHW",ms_0312)]
    return []
@to_imagedef
def change_arrange(imdef:ImageDef):
    match TensorLike(dtype,arng,ch_repr,vr) in imdef:
        return [DataEdge(imdef,
                         imdef.__class__(dtype,_arng,ch_repr,vr),
                         f,
                         1,
                         name=f"{arng} to {_arng}",
                         meta_shifter=meta_shifter) for f,_arng,meta_shifter in change_arng(imdef)]
    return []
ms_drop_bhwc_alpha = (s->(s[0],s[1],s[2],3)) |> ss_to_ms$
ms_drop_bchw_alpha = (s->(s[0],3,s[2],s[3])) |> ss_to_ms$
@to_imagedef
def drop_alpha(imdef):
    case imdef:
        match TensorLike(dtype,"BHWC","RGBA",vr):
            return [DataEdge(a=imdef,
                         b=imdef.__class__(dtype,"BHWC","RGB",vr),
                         f=a->a[:,:,:,:3],
                         cost=1,
                         name=f"select rgb channel",
                         meta_shifter=ms_drop_bhwc_alpha
                         )]
        match TensorLike(dtype,"BCHW","RGBA",vr):
            return [DataEdge(a=imdef,
                         b=imdef.__class__(dtype,"BCHW","RGB",vr),
                         f=a->a[:,:3],
                         cost=1,
                         name=f"select rgb channel",
                         meta_shifter=ms_drop_bchw_alpha
                         )]

ms_select_bhwc_channel = (s->(s[0],s[1],s[2],1)) |> ss_to_ms$
ms_select_bchw_channel = (s->(s[0],1,s[2],s[3])) |> ss_to_ms$

@to_imagedef
def select_channel(imdef:ImageDef):
    case imdef:
        match TensorLike(dtype,"BHWC",ch_repr,vr) if len(ch_repr) >= 1:
            selector = i->a->a[:,:,:,[i]]
            return [DataEdge(a=imdef,
                         b=imdef.__class__(dtype,"BHWC",c,vr),
                         f=selector(i),
                         cost=10,
                         name=f"select {c} channel",
                         meta_shifter=ms_select_bhwc_channel
                         ) for i,c in enumerate(ch_splitter(ch_repr))]
        match TensorLike(dtype,"BCHW",ch_repr,vr) if len(ch_repr) >= 1:
            selector = i->a->a[:,[i]]
            return [DataEdge(a=imdef,
                         b=imdef.__class__(dtype,"BCHW",c,vr),
                         f=selector(i),
                         cost=10,
                         name=f"select {c} channel",
                         meta_shifter=ms_select_bchw_channel
                         ) for i,c in enumerate(ch_splitter(ch_repr))]
    return []
ms_drop_bhwc_channel = (s->(s[0],s[1],s[2])) |> ss_to_ms$
ms_drop_bchw_channel = (s->(s[0],s[2],s[3])) |> ss_to_ms$
ms_drop_chw_channel = (s->(s[1],s[2])) |> ss_to_ms$
ms_drop_hwc_channel = (s->(s[0],s[1])) |> ss_to_ms$


@to_imagedef
def drop_channel(imdef:ImageDef):
    case imdef:
        match TensorLike(dtype,="BHWC",ch_repr,vr) if len(ch_splitter(ch_repr)) == 1:
            return [DataEdge(a=imdef,
                        b=imdef.__class__(dtype,"BHW",ch_repr,vr),
                        f=a->a[:,:,:,0],
                        cost = 1,
                        name=f"BHWC to BHW",
                        meta_shifter=ms_drop_bhwc_channel
                       )]
        match TensorLike(dtype,"BCHW",ch_repr,vr) if len(ch_splitter(ch_repr)) == 1:
            return [DataEdge(a=imdef,
                        b=imdef.__class__(dtype,"BHW",ch_repr,vr),
                        f=a->a[:,0],
                        cost = 1,
                        name=f"BCHW to BHW",
                        meta_shifter=ms_drop_bchw_channel
                       )]
        match TensorLike(dtype,"CHW",ch_repr,vr) if len(ch_splitter(ch_repr)) == 1:
            return [DataEdge(a = imdef,b=imdef.__class__(dtype,"HW",ch_repr,vr),
                         f = a->a[0],cost=1,name="CHW to HW",
                         meta_shifter=ms_drop_chw_channel
            )]
        match TensorLike(dtype,"HWC",ch_repr,vr) if len(ch_splitter(ch_repr)) == 1:
            return [DataEdge(a = imdef,b=imdef.__class__(dtype,"HW",ch_repr,vr),
                         f = a->a[:,:,0], cost=1,name="HWC to HW",
                         meta_shifter=ms_drop_hwc_channel
            )]

    return []
def enforce_mode(img,mode):
    return Image.fromarray(np.array(img),mode)
"""
@to_imagedef
def RGB_to_YCbCr(state):
    case state:
        match PILImage("RGB","RGB"):
            return [DataEdge(
            a=state,
            b=PILImage("YCbCr","YCbCr"),
            f= enforce_mode$(mode="RGB") ..> .convert("YCbCr"),
            cost=1,
            name="RGB to YCbCr"
            )]
        match PILImage("YCbCr","YCbCr"):
            return [DataEdge(
            a=state,
            b=PILImage("RGB","RGB"),
            f= enforce_mode$(mode="YCbCr") ..> .convert("RGB"),
            cost=1,
            name="YCbCr to RGB"
            )]
"""
def rgb_to_ycbcr(image: torch.Tensor) -> torch.Tensor:
    r"""Convert an RGB image to YCbCr.

    Args:
        image (torch.Tensor): RGB Image to be converted to YCbCr with shape :math:`(*, 3, H, W)`.

    Returns:
        torch.Tensor: YCbCr version of the image with shape :math:`(*, 3, H, W)`.

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = rgb_to_ycbcr(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(image)))

    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError("Input size must have a shape of (*, 3, H, W). Got {}"
                         .format(image.shape))

    r: torch.Tensor = image[..., 0, :, :]
    g: torch.Tensor = image[..., 1, :, :]
    b: torch.Tensor = image[..., 2, :, :]

    delta: float = 0.5
    y: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b
    cb: torch.Tensor = (b - y) * 0.564 + delta
    cr: torch.Tensor = (r - y) * 0.713 + delta
    return torch.stack([y, cb, cr], -3)


def ycbcr_to_rgb(image: torch.Tensor) -> torch.Tensor:
    r"""Convert an YCbCr image to RGB.

    The image data is assumed to be in the range of (0, 1).

    Args:
        image (torch.Tensor): YCbCr Image to be converted to RGB with shape :math:`(*, 3, H, W)`.

    Returns:
        torch.Tensor: RGB version of the image with shape :math:`(*, 3, H, W)`.

    Examples:
        >>> input = torch.rand(2, 3, 4, 5)
        >>> output = ycbcr_to_rgb(input)  # 2x3x4x5
    """
    if not isinstance(image, torch.Tensor):
        raise TypeError("Input type is not a torch.Tensor. Got {}".format(
            type(image)))

    if len(image.shape) < 3 or image.shape[-3] != 3:
        raise ValueError("Input size must have a shape of (*, 3, H, W). Got {}"
                         .format(image.shape))

    y: torch.Tensor = image[..., 0, :, :]
    cb: torch.Tensor = image[..., 1, :, :]
    cr: torch.Tensor = image[..., 2, :, :]

    delta: float = 0.5
    cb_shifted: torch.Tensor = cb - delta
    cr_shifted: torch.Tensor = cr - delta

    r: torch.Tensor = y + 1.403 * cr_shifted
    g: torch.Tensor = y - 0.714 * cr_shifted - 0.344 * cb_shifted
    b: torch.Tensor = y + 1.773 * cb_shifted
    return torch.stack([r, g, b], -3)


@to_imagedef
def RGB_to_YCbCr(state):
    case state:
        match Torch("float32","BCHW","RGB",=VR_0_1):
            return [DataEdge(a=state,
                         b=Torch("float32","BCHW","RGB",VR_0_1),
                         f=rgb_to_ycbcr,
                         cost=1,
                         name="RGB_to_YCbCr(torch)"
            )]
        match Torch("float32","BCHW","YCbCr",=VR_0_1):
            return [DataEdge(a=state,
                         b=Torch("float32","BCHW","RGB",VR_0_1),
                         f=ycbcr_to_rgb,
                         cost=1,
                         name="YCbCr_to_RGB(torch)"
                        )]
        match Torch("float32","CHW","RGB",=VR_0_1):
            return [DataEdge(a=state,
                         b=Torch("float32","CHW","YCbCr",VR_0_1),
                         f=rgb_to_ycbcr,
                         cost=1,
                         name="RGB_to_YCbCr(torch)"
            )]
        match Torch("float32","CHW","YCbCr",=VR_0_1):
            return [DataEdge(a=state,
                         b=Torch("float32","CHW","RGB",VR_0_1),
                         f=ycbcr_to_rgb,
                         cost=1,
                         name="YCbCr_to_RGB(torch)"
                        )]



#shape change!
ms_add_b_ch = (s->(1,*s)) |> ss_to_ms$
"adds 1 as batch shape"
ms_del_b_ch = (s->s[1:])  |> ss_to_ms$
# TODO what to do with non-batched states?

def en_batch(imdef:ImageDef):
    case imdef:
        match ImageDef(TensorLike(dtype,"HWC" or "CHW" or "HW",ch_repr,vr),meta):
            new_arng = "B"+imdef.data_type.arrange
            return [Edge(a=imdef,
                         b=ImageDef(imdef.data_type.__class__(dtype,new_arng,ch_repr,vr),ms_add_b_ch(meta)),
                         f=a->a[None],
                         cost=10,
                         name=f"tensor_like en_batch"
                        )]
        match ImageDef(PILImage(mode,channel_repr),meta):
            return [Edge(a=imdef,
                         b=ImageDef(PILImages(mode,channel_repr),ms_add_b_ch(meta)),
                         f=a->[a],
                         cost=10,
                         name=f"pil image en_batch"
                        )]
    return []

def de_batch(imdef:ImageDef):
    case imdef:
        match ImageDef(TensorLike(dtype,arng,ch,vr),{"shape":shape} as meta) if "B" in arng and shape[0] == 1:
            return [Edge(
                a=imdef,
                b=ImageDef(imdef.data_type.__class__(dtype,arng[1:],ch,vr),ms_del_b_ch(meta)),
                f=a->a[0],
                cost=1,
                name=f"de_batch en_batched image"
            )]
        match ImageDef(PILImages(mode,ch),{"shape":shape} as meta) if "en_batched" in meta:
            return [Edge(
                a=imdef,
                b=ImageDef(PILImage(mode,ch),ms_del_b_ch(meta)),
                f=a->a[0],
                cost=1,
                name=f"de_batch en_batched image"
            )]

def drop_meta(imdef:ImageDef):
    case imdef:
        match ImageDef(data_type,meta):
            return [Edge(
                a=imdef,
                b=ImageDef(data_type,fdict()),
                f=a->a,
                cost=1,
                name="drop all metadata"
            )]


@to_imagedef
def to_rgba(imdef:ImageDef):
    case imdef:
        match TensorLike(dtype,arng,ch_repr,"0_1") if len(ch_splitter(ch_repr)) == 4:
            return [DataEdge(a=imdef,
                         b=imdef.__class__(dtype,arng,"RGBA","0_1"),
                         f=a->a,
                         cost=10,
                         name=f"view {ch_repr} as RGBA "
                        )]
        match TensorLike(dtype,arng,ch_repr,"0_1") if len(ch_splitter(ch_repr)) == 3:
            return [DataEdge(a=imdef,
                         b=imdef.__class__(dtype,arng,"RGB","0_1"),
                         f=a->a,
                         cost=10,
                         name=f"view {ch_repr} as RGB "
                        )]
@to_imagedef
def change_value_range(imdef:ImageDef):
    case imdef:
        match TensorLike("float32" or "float64",arng,ch_repr,=VR_0_255):
            return [DataEdge(a=imdef,
                         b=imdef.__class__(imdef.dtype,arng,ch_repr,VR_0_1),
                         f=a->a/255.0,
                         cost=len(ch_repr),
                         name="0-255 to 0-1"
                        )]
        match TensorLike("float32" or "float64",arng,ch_repr,=VR_0_1):
            return [DataEdge(a=imdef,
                         b=imdef.__class__(imdef.dtype,arng,ch_repr,VR_0_255),
                         f=a->a*255.0,
                         cost=len(ch_repr),
                         name="0-1 to 0-255"
                        )]
    return []

def xyza_to_rgba(xyza):
    xyz = xyza[:3]
    a = xyza[[3]]
    rgb = (xyz+1)/2
    return np.concatenate((rgb,a),axis=0)
def xyz_to_rgb(xyz):
    return (xyz+1)/2
def rgb_to_xyz(rgb):
    return (rgb*2)-1
def rgba_to_xyza(rgba):
    rgb = rgba[:3]
    a = rgba[[3]]
    xyz = (rgb*2)-1
    return np.concatenate((xyz,a),axis=0)

def rule_xyz_to_rgb(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,"CHW","XYZA","-1_1"),meta):
            return [
                (xyza_to_rgba,ImageDef(Numpy(dtype,"CHW","RGBA",VR_0_1),meta),2,"xyza_to_rgba")
            ]
        match ImageDef(Numpy(dtype,"CHW","XYZ","-1_1"),meta):
            return [
                (xyz_to_rgb,ImageDef(Numpy(dtype,"CHW","RGB",VR_0_1),meta),2,"xyz_to_rgb")
            ]
        match ImageDef(Numpy(dtype,"CHW","RGBA",=VR_0_1),meta):
            return [
                (rgba_to_xyza,ImageDef(Numpy(dtype,"CHW","XYZA","-1_1"),meta),2,"rgba_to_xyza")
            ]
        match ImageDef(Numpy(dtype,"CHW","RGB",=VR_0_1),meta):
            return [
                (rgb_to_xyz,ImageDef(Numpy(dtype,"CHW","XYZ","-1_1"),meta),2,"rgb_to_xyz")
            ]

def b_xyza_to_rgba(xyza):
    xyz = xyza[:,:3]
    a = xyza[:,[3]]
    rgb = (xyz+1)/2
    return np.concatenate((rgb,a),axis=1)
def b_xyz_to_rgb(xyz):
    return (xyz+1)/2
def b_rgb_to_xyz(rgb):
    return (rgb*2)-1
def b_rgba_to_xyza(rgba):
    rgb = rgba[:,:3]
    a = rgba[:,[3]]
    xyz = (rgb*2)-1
    return np.concatenate((xyz,a),axis=1)

def rule_batch_xyz_to_rgb(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,"BCHW","XYZA","-1_1"),meta):
            return [
                (b_xyza_to_rgba,ImageDef(Numpy(dtype,"BCHW","RGBA",VR_0_1),meta),2,"xyza_to_rgba(batch)")
            ]
        match ImageDef(Numpy(dtype,"BCHW","XYZ","-1_1"),meta):
            return [
                (b_xyz_to_rgb,ImageDef(Numpy(dtype,"BCHW","RGB",VR_0_1),meta),2,"xyz_to_rgb(batch)")
            ]
        match ImageDef(Numpy(dtype,"BCHW","RGBA",=VR_0_1),meta):
            return [
                (b_rgba_to_xyza,ImageDef(Numpy(dtype,"BCHW","XYZA","-1_1"),meta),2,"rgba_to_xyza(batch)")
            ]
        match ImageDef(Numpy(dtype,"BCHW","RGB",=VR_0_1),meta):
            return [
                (b_rgb_to_xyz,ImageDef(Numpy(dtype,"BCHW","XYZ","-1_1"),meta),2,"rgb_to_xyz(batch)")
            ]



_conversions =[
    to_PILImages,
    to_numpy,
    to_torch,
    change_dtype,
    change_arrange,
    select_channel,
    drop_channel,
    en_batch,
    change_value_range,
    drop_alpha,
    to_rgba,
    drop_meta,
    de_batch,
    RGB_to_YCbCr,
]


@memoize(1024)
def _edges(imdef):
    res = []
    for f in _conversions:
        edges = f(imdef)
        if edges is not None:
            res += edges
    return res




@memoize(1024)
def str_to_img_def(query):
    """
    ex1: 'numpy,float32,BCHW,RGB,0_255 | hello,world'
    ex2: 'torch,float32,BCHW,RGBA,0_1'
    ex3: 'image,RGBA,RGBA'
    ex4: 'images,RGB,RGB' => ImageDef(PILImage("RGB","RGB"),{shape:(None,None,3)})
    ex5: 'image,RGBA,RGBA,128:128:3' => ImageDef(PILImage("RGB","RGB"),{shape:(128,128,3)})
    """
    vrs = {
        "0_255":VR_0_255,
        "0_1":VR_0_1,
        "None":VR_None
    }
    query = query.replace(" ","")
    def arng_to_shape(arng,ch_repr):
        ch = len(ch_splitter(ch_repr))
        c_idx = arng.find("C")
        shape = [None]*len(arng)
        if c_idx != -1:
            shape[c_idx] = ch
        #logger.info(f"{arng},{ch_repr}->{tuple(shape)}")
        return tuple(shape)

    def shape_str_to_shape(ss):
        tokens = ss.split(":")
        return tuple([int(t) if t!="None" else None for t in tokens])

    def query_to_data_type(query):
        case query.split(","):
            match ["numpy",dtype,arng,ch,vr]:
                return Numpy(dtype,arng,ch,vrs[vr] if vr in vrs else vr),arng_to_shape(arng,ch)
            match ["numpy",dtype,arng,ch,vr,shape]:
                return Numpy(dtype,arng,ch,vrs[vr] if vr in vrs else vr),shape_str_to_shape(shape)
            match ["torch",dtype,arng,ch,vr]:
                return Torch(dtype,arng,ch,vrs[vr] if vr in vrs else vr),arng_to_shape(arng,ch)
            match ["torch",dtype,arng,ch,vr,shape]:
                return Torch(dtype,arng,ch,vrs[vr] if vr in vrs else vr),shape_str_to_shape(shape)
            match ["image",mode,ch] if len(ch_splitter(ch)) > 1:
                return PILImage(mode,ch),(None,None,len(ch_splitter(ch)))
            match ["image",mode,ch] if len(ch_splitter(ch)) ==1:
                return PILImage(mode,ch),(None,None)
            match ["image",mode,ch,shape]:
                return PILImage(mode,ch),shape_str_to_shape(shape)
            match ["images",mode,ch] if len(ch_splitter(ch)) > 1:
                return PILImages(mode,ch),(None,None,None,len(ch_splitter(ch)))
            match ["images",mode,ch] if len(ch_splitter(ch)) == 1:
                return PILImages(mode,ch),(None,None,None)
            match ["images",mode,ch,shape]:
                return PILImages(mode,ch),shape_str_to_shape(shape)
    case query_to_data_type(query):
        match (data_type,shape):
            return ImageDef(data_type,fdict(shape=shape))
    else:
        raise RuntimeError(f"could not parse image def string!:{query}")

def parse_def(img_def):
    try:
        return str_to_img_def(img_def) if img_def `isinstance` str else img_def
    except Exception as e:
        return img_def



accept_def_str = f -> parse_def ..> f
def imdef_neighbors(imdef):
    return [(e.f,e.b,e.cost,e.name) for e in _edges(imdef)]

#from data_tree.coconut.convert import AutoImage,PILImage,str_to_img_def,PILImages
#from data_tree.coconut.convert import ImageDef,Torch,Numpy,TensorLike,VR_0_1,VR_None,VR_0_255

def normalize_numpy_img(ary):
    _min = ary.min()
    _max = ary.max()
    return ((ary-_min)/(_max-_min))

def rule_VR_None_to_normalized(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,("CHW" or "HW") as arng,ch,=VR_None),meta):
            return [(
                normalize_numpy_img,
                ImageDef(Numpy(dtype,arng,ch,VR_0_1),meta),
                1,
                "minmax_0_1_numpy_img"
            )]
        match ImageDef(Numpy(dtype,"BCHW",ch,=VR_None),meta):
            return [(
                batch->np.array([normalize_numpy_img(img) for img in batch]),
                ImageDef(Numpy(dtype,"BCHW",ch,VR_0_1),meta),
                1,
                "batch_minmax_0_1_numpy_img"
            )]
ms_add_ch_to_hw = (s->(1,s[0],s[1])) |> ss_to_ms$
ms_add_ch_to_bhw = (s->(s[0],1,s[1],s[2])) |> ss_to_ms$
def rule_add_channel(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,"HW",ch,vr),meta):
            return [(
                a->a[None],
                ImageDef(Numpy(dtype,"CHW",ch,vr),meta|>ms_add_ch_to_hw),
                1,
                "add_channel_dim"
            )]
        match ImageDef(Numpy(dtype,"BHW",ch,vr),meta):
            return [(
                a->a[:,None],
                ImageDef(Numpy(dtype,"BCHW",ch,vr),meta|>ms_add_ch_to_bhw),
                1,
                "add_channel_dim"
            )]
def rule_swap_RGB_BGR(imdef):
    case imdef:
        match ImageDef(TensorLike(dtype,"BHWC",("RGB" or "BGR") as rgb_order,vr) as tl,meta):
            return [(
                a->a[:,:,:,[2,1,0]],
                ImageDef(tl.__class__(dtype,"BHWC","RGB" if rgb_order.startswith("B") else "BGR",vr),meta),
                1,
                "swap rgb or bgr"
            )]
def rule_BGR_to_LAB(imdef):
    from skimage import color
    case imdef:
        match ImageDef(Numpy("float32","HWC","BGR",=VR_0_1),meta):
            return[(
                color.rgb2lab,
                ImageDef(Numpy("float32","HWC","LAB","VR_LAB"),meta),
                1,
                "bgr_0_1 to lab"
            )]
        match ImageDef(Numpy("float32","HWC","LAB","VR_LAB"),meta):
            return [(
                color.lab2rgb,
                ImageDef(Numpy("float32","HWC","BGR",VR_0_1),meta),
                1,
                "lab to bgr_0_1"
            )]




class AutoImage:
    default_rules = [
        imdef_neighbors,
        rule_xyz_to_rgb,
        rule_batch_xyz_to_rgb,
        rule_VR_None_to_normalized,
        rule_add_channel,
        rule_swap_RGB_BGR,
        rule_BGR_to_LAB
        ]
    solver = AStarSolver(rules=default_rules.copy())

    @staticmethod
    def reset_solver():
        AutoImage.solver = AStarSolver(rules = AutoImage.default_rules.copy())

    @staticmethod
    def debug_conversion(a,b,samples):
        x = samples
        edges = AutoImage.solver.search_direct(a,b).edges
        for edge in edges:
            print(edge)
            print(edge.f)
            x = edge.f(x)
            print(f"converted to type:{type(x)}")
            if x `isinstance` np.ndarray:
                print(x.shape)
            print(f"converted:{x}")
        return x

    def to_debug(self,img_def):
        img_def = parse_def(img_def)
        return AutoImage.debug_conversion(self.img_def,img_def,self.data)

    def __init__(self,data,img_def):
        img_def = parse_def(img_def)
        self.data = data
        self.img_def = img_def

    def converter(self,img_def):
        img_def = parse_def(img_def)
        return AutoImage.solver.search_direct(self.img_def,img_def)

    def any_converter(self,img_defs):
        imdefs = [parse_def(imdef) for imdef in img_defs]
        return AutoImage.solver.search_direct_any(self.img_def,imdefs)

    def convert(self,img_def):
        convert = self.converter(img_def)
        if convert.edges:
            return AutoImage(convert(self.data),convert.edges[-1].dst)
        else:
            return self

    def any_convert(self,imdefs):
        converter = self.any_converter(imdefs)
        if converter.edges:
            return AutoImage(converter(self.data),converter.edges[-1].dst)
        else:
            return self

    def to(self,img_def is (str,ImageDef),log_trace=False):
        return self.convert(img_def).data

    def any_to(self,imdefs):
        return self.any_convert(imdefs).data

    def to_widget(self):
        case self.img_def.data_type:
            match item is PILImages:
                return self.tile_image().to_widget()
            match TensorLike(_,arng,*_) if "B" in arng:
                return self.tile_image().to_widget()
        else:
            convert = self.converter(self.to_images_def())
        return convert(self.data) |> infer_widget

    def _repr_html_(self):
        return self.to_widget() |> display

    def to_images_def(self):
        """
        you have to add en_batched tag when data is not batch.
        """
        tag_opt=frozenset()
        img_cls = PILImages
        case self.img_def:
            match ImageDef(TensorLike(_,arng,_,_),meta) if "B" not in arng:
                tag_opt = frozenset(("en_batched",))
            match ImageDef(PILImage(mode,ch),meta):
                tag_opt = frozenset(("en_batched",))

        case self.img_def.data_type:
            match PILImage(mode,chrepr):
                return ImageDef(PILImages(mode,chrepr),self.img_def.meta | tag_opt)
            match PILImages(mode,chrepr):
                return self.img_def
            match TensorLike(dtype,arng,c,vr) if len(c) == 1:
                return ImageDef(img_cls("L",c),self.img_def.meta | tag_opt)
            match TensorLike(dtype,arng,"RGBA",vr):
                return ImageDef(img_cls("RGBA","RGBA"),self.img_def.meta | tag_opt)
            match TensorLike(dtype,arng,"RGB",vr):
                return ImageDef(img_cls("RGB","RGB"),self.img_def.meta | tag_opt)
            match TensorLike(dtype,arng,ch,vr) if "A" in ch and ch != "LAB":
                return ImageDef(img_cls("RGBA","RGBA"),self.img_def.meta | tag_opt)
        else:
            attempts=[
                "image,RGBA,RGBA",
                "images,RGBA,RGBA",
                "image,RGB,RGB",
                "images,RGB,RGB",
                "image,L,L",
                "images,L,L"
            ]
            for tgt in attempts:
                imdef = str_to_img_def(tgt)
                try:
                    converter = self.converter(imdef)
                    return imdef
                except NoRouteException as e:
                    #logger.warning(f"no route for:{imdef}. trying next imdef.")
                    pass
            raise RuntimeError(f"cannot convert to image:{self.img_def} to any image_like imdef}")


    def image_op(self,f:Image->Image):
        images_def = self.to_images_def()
        images = self.to(images_def)
        new_images=[f(i) for i in images ] # do some resizing or something
        new_ai = AutoImage(new_images,images_def)
        return new_ai.convert(self.img_def) # go back to last state.

    def visdom(self,visdom=None,**kwargs):
        if visdom is None:
            from data_tree.visdom import VISDOM
            visdom = VISDOM
        candidates = [
           "numpy,float32,CHW,RGB,0_1",
           "numpy,float32,CHW,L,0_1",
           "numpy,float32,BCHW,RGB,0_1",
           "numpy,float32,BCHW,L,0_1"
        ]
        img = self.any_convert(candidates)
        data_type = img.img_def.data_type
        case data_type:
           match Numpy(_,"CHW",_,_):
               res = visdom.image(img.data,**kwargs)
           match Numpy(_,"BCHW",_,_):
               res = visdom.images(img.data,**kwargs)
        return res


@property
def AutoImage.images(self):
    return self.to(self.to_images_def())

@property
def AutoImage.image_size(self):
    return self.images[0].size

def make_grid(imgs, nrow, padding=0):
    """Numpy配列の複数枚の画像を、1枚の画像にタイルします

    Arguments:
        imgs {np.ndarray} -- 複数枚の画像からなるテンソル
        nrow {int} -- 1行あたりにタイルする枚数

    Keyword Arguments:
        padding {int} -- グリッドの間隔 (default: {0})

    Returns:
        [np.ndarray] -- 3階テンソル。1枚の画像
    """
    assert imgs.ndim == 4 and nrow > 0
    batch, height, width, ch = imgs.shape
    n = nrow * (batch // nrow + np.sign(batch % nrow))
    ncol = n // nrow
    pad = np.zeros((n - batch, height, width, ch), imgs.dtype)
    x = np.concatenate([imgs, pad], axis=0)
    # border padding if required
    if padding > 0:
        x = np.pad(x, ((0, 0), (0, padding), (0, padding), (0, 0)),
                   "constant", constant_values=(0, 0)) # 下と右だけにpaddingを入れる
        height += padding
        width += padding
    x = x.reshape(ncol, nrow, height, width, ch)
    x = x.transpose([0, 2, 1, 3, 4])  # (ncol, height, nrow, width, ch)
    x = x.reshape(height * ncol, width * nrow, ch)
    if padding > 0:
        x = x[:(height * ncol - padding),:(width * nrow - padding),:] # 右端と下端のpaddingを削除
    return x


def AutoImage.tile_image(self,w=1024,h=1024,max_image=100,padding=1):
    ch = self.to_images_def().data_type.channel_repr
    if len(ch) == 1:
        codec = f"numpy,uint8,BHW,{ch},0_255"
    else:
        codec = f"numpy,uint8,BHWC,{ch},0_255"
    imgs = self.to(codec)[:max_image]
    nrow = int(sqrt(len(imgs))+0.5)
    r = int((w-((nrow+1)*padding))/nrow)
    imgs = np.array([(Image.fromarray(img).resize((r,r)) |> np.array) for img in imgs])
    if len(ch) == 1:
        imgs = imgs[:,:,:,None]
    return AutoImage(make_grid(imgs,nrow,padding=1),f"numpy,uint8,HWC,{ch},0_255")

img_to_shifting_grids = img->make_grids(*img.image_size)|> shifting_grids
def auto_to_3res(img:"AutoImage",cx,cy,r=256)->"AutoImage":
    img = img.to("image,L,L")
    #img = img.resize((2048,2048))
    chs = [crop_square(img,cx,cy,_r).resize((r,r)) for _r in [r*4,r*2,r]]
    return AutoImage(np.concatenate([np.array(i)[:,:,None] for i in chs],axis=2),"numpy,float32,HWC,RGB,0_255")

def img_to_grid_batch(img:AutoImage):
    grids = img_to_shifting_grids(img) |> .astype("int32") |> series
    batch = grids.map((xy)->auto_to_3res(img,xy[0]+128,xy[1]+128,r=256).to("numpy,float32,HWC,RGB,0_1")).values |> array |> .astype("float32")
    return grids.values,AutoImage(batch,"numpy,float32,BHWC,RGB,0_1")


def AutoImage.cast(self,imgdef):
    return AutoImage(self.data,imgdef)


