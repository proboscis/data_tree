from PIL import Image
import numpy as np
import heapq
from data_tree.coconut.visualization import infer_widget
from loguru import logger
from data_tree.coconut.astar import new_conversion,AStarSolver,NoRouteException
from math import sqrt
data ValueRange(name):
    def __str__(self):
        return self.name
VR_0_1 = ValueRange("0_1")
VR_0_255 = ValueRange("0_255")
VR_None = ValueRange("None")
VR_XYZ_Normalized = ValueRange("XYZ_Normalized")


data DataType
#TODO add shape information to tensorlike
#TODO add shape information to PILImage

data TensorLike(
    dtype is str,
    arrange is str,
    channel_repr is str,
    value_range is ValueRange) from DataType:
    def __repr__(self):
        return f"Tensor({self.data_type}|{self.dtype}|{self.arrange}|{self.channel_repr}|{self.value_range})"

data Numpy(dtype,arrange,channel_repr,value_range) from TensorLike:
    def __new__(cls,*args):
        return makedata(cls,*args)
    def __repr__(self):
        return f"Numpy({self.dtype},{self.arrange},{self.channel_repr},{self.value_range})"

data Torch(dtype,arrange,channel_repr,value_range) from TensorLike:
    def __new__(cls,*args):
        return makedata(cls,*args)
    def __repr__(self):
        return f"Torch({self.dtype},{self.arrange},{self.channel_repr},{self.value_range})"

data PILImages(mode,channel_repr) from DataType: # represents iterable of PIL.Images
    def __repr__(self):
        return f"PILImages({self.mode},{self.channel_repr})"
data PILImage(mode,channel_repr) from DataType:
    def __repr__(self):
        return f"PILImage({self.mode},{self.channel_repr})"

data ImageDef(data_type is DataType,tags is frozenset):
    def __repr__(self):
        return f"ImageDef({self.data_type}|{list(self.tags)})"


DTYPES={"float32","float64","int32","int64","uint8","bool"}
data Edge(a:ImageDef,b:ImageDef,f,cost:int,name="undefined"):
    def __repr__(self):
        return f"{self.a} \t-> {self.name}\t-> {self.b}"
from typing import List

#純粋にエッジだけを考えると、どうしても組み合わせ爆発になる。目的地を知っていれば削減できる。

# 各imdefのNodeベースでエッジを定義するのか。それとも。エッジのルールを網羅するのか。
# オペレーターのほうが数が少ないので、オペレーターだけ定義したい。
# List up operators.
# operator definition: ImageDef->List[Edge]
# 1. change data_type
# 2. change dtype
# 3. change arrange
# 4. change ch_repr
# 5. select channel
def to_imagedef(f):
    def _inner(imdef:ImageDef):
        try:
            #logger.debug(type(imdef))
            if imdef `isinstance` ImageDef and len(imdef) >= 1 and imdef `hasattr` "data_type":
            #if imdef `isinstance` ImageDef:
                edges = f(imdef.data_type)
                if edges is not None:
                    return [Edge(imdef,ImageDef(e.b,imdef.tags),e.f,e.cost,e.name) for e in edges]
                else:
                    return []
            else:
                return []
        except Exception as e:
            logger.warning(f"unknown error...imdef:{imdef}")
            logger.warning(f"{imdef} has attr causes exception?")
            logger.warning(f"{hasattr(imdef,'data_type')}")
            raise e
    return _inner

@to_imagedef
def to_PILImages(imdef:ImageDef)->Edge[]:
    #TODO fix pattern match on data class
    case imdef:
        match Numpy("uint8","BHWC",("RGBA" or "RGB" or "LAB") as c_repr, =VR_0_255):
            return [Edge(imdef,PILImages(c_repr,c_repr),ary -> [(Image.fromarray ..> .convert(c_repr))(img) for img in ary],2,name="to_Images")]
        match Numpy("uint8","BHW",c_repr,=VR_0_255) if len(c_repr) == 1:
            return [Edge(imdef,PILImages("L",c_repr),ary -> [(Image.fromarray ..> .convert("L"))(img) for img in ary],2,name="to_Images")]
    return []
@to_imagedef
def to_numpy(imdef:ImageDef)->List[Edge]:
    case imdef:
        match Torch(dtype,arng,ch_repr,vr): # torch tensor can always become numpy
            return [Edge(imdef,
                         Numpy(dtype  ,arng ,ch_repr,vr),
                         (.detach() ..> .cpu() ..> .numpy()),
                         1,
                         name="torch_to_numpy")]
        match PILImages("L",ch_repr): # A grayscale Image becomes a numpy array
            return [Edge(imdef,
                         Numpy("uint8","BHW",ch_repr,VR_0_255),
                         (fmap$(np.array) ..> np.array),
                         1,
                         name="image_to_numpy")]
        match PILImages(mode,ch_repr):# A multi-channel Image becomes a numpy array
            return [Edge(imdef,
                         Numpy("uint8","BHWC",ch_repr,VR_0_255),
                         (fmap$(np.array) ..> np.array),
                         1,
                         name="image_to_numpy")]
    return []
@to_imagedef
def to_torch(imdef:ImageDef):
    import torch
    case imdef:
        match Numpy(dtype,arng,ch_repr,vr): # only numpy can directly become a torch tensor
            return [Edge(imdef,Torch(dtype,arng,ch_repr,vr),torch.from_numpy,2,name="to_torch")]
    return []
@to_imagedef
def change_dtype(imdef:ImageDef):# TODO match value range to dtype with bool type
    case imdef:
        match Numpy(dtype,arng,ch_repr,vr):
            return [Edge(
                imdef,
                imdef.__class__(_dtype,arng,ch_repr,vr),
                .astype(_dtype),
                1,
                name=f"{dtype} to {_dtype}"
            ) for _dtype in DTYPES if _dtype != dtype]
    return []


def change_arng(imdef):
    case imdef:
        match Numpy(_,"BCHW",_,_):
            return [(.transpose(0,2,3,1),"BHWC")]
        match Numpy(_,"BHWC",_,_):
            return [(.transpose(0,3,1,2),"BCHW")]
        match Torch(_,"BCHW",_,_):
            return [(.transpose(1,2) ..> .transpose(2,3),"BHWC")]
        match Torch(_,"BHWC",_,_):
            return [(.transpose(2,3) ..> .transpose(1,2),"BCHW")]
    return []

@to_imagedef
def drop_alpha(imdef):
    case imdef:
        match TensorLike(dtype,"BHWC","RGBA",vr):
            return [Edge(a=imdef,
                         b=imdef.__class__(dtype,"BHWC","RGB",vr),
                         f=a->a[:,:,:,:3],
                         cost=1,
                         name=f"select rgb channel")]
        match TensorLike(dtype,"BCHW","RGBA",vr):
            return [Edge(a=imdef,
                         b=imdef.__class__(dtype,"BCHW","RGB",vr),
                         f=a->a[:,:3],
                         cost=1,
                         name=f"select rgb channel")]
@to_imagedef
def change_arrange(imdef:ImageDef):
    match TensorLike(dtype,arng,ch_repr,vr) in imdef:
        return [Edge(imdef,imdef.__class__(dtype,_arng,ch_repr,vr),f,1,name=f"{arng} to {_arng}") for f,_arng in change_arng(imdef)]
    return []

@to_imagedef
def select_channel(imdef:ImageDef):
    case imdef:
        match TensorLike(dtype,"BHWC",ch_repr,vr) if len(ch_repr) >= 1:
            return [Edge(a=imdef,
                         b=imdef.__class__(dtype,"BHWC",c,vr),
                         f=a->a[:,:,:,[i]],
                         cost=1,
                         name=f"select {c} channel") for i,c in enumerate(ch_repr)]
        match TensorLike(dtype,"BCHW",ch_repr,vr) if len(ch_repr) >= 1:
            return [Edge(a=imdef,
                         b=imdef.__class__(dtype,"BHWC",c,vr),
                         f=a->a[:,[i]],
                         cost=1,
                         name=f"select {c} channel") for i,c in enumerate(ch_repr)]
    return []
@to_imagedef
def drop_channel(imdef:ImageDef):
    case imdef:
        match TensorLike(dtype,="BHWC",ch_repr,vr) if len(ch_repr) == 1:
            return [Edge(a=imdef,
                        b=imdef.__class__(dtype,"BHW",ch_repr,vr),
                        f=a->a[:,:,:,0],
                        cost = 1,
                        name=f"BHWC to BHW"
                       )]
        match TensorLike(dtype,"BCHW",ch_repr,vr) if len(ch_repr) == 1:
            return [Edge(a=imdef,
                        b=imdef.__class__(dtype,"BHW",ch_repr,vr),
                        f=a->a[:,0],
                        cost = 1,
                        name=f"BCHW to BHW"
                       )]
    return []


def en_batch(imdef:ImageDef):
    case imdef:
        match ImageDef(TensorLike(dtype,"HWC" or "CHW" or "HW",ch_repr,vr),tags):
            new_arng = "B"+imdef.data_type.arrange
            return [Edge(a=imdef,
                         b=ImageDef(imdef.data_type.__class__(dtype,new_arng,ch_repr,vr),tags|frozenset(("en_batched",))),
                         f=a->a[None],
                         cost=1,
                         name=f"{imdef.data_type.arrange} to {new_arng}"
                        )]
        match ImageDef(PILImage(mode,channel_repr),tags):
            return [Edge(a=imdef,
                         b=ImageDef(PILImages(mode,channel_repr),tags|frozenset(("en_batched",))),
                         f=a->[a],
                         cost=1,
                         name=f"wrap image with list"
                        )]
    return []
def de_batch(imdef:ImageDef):
    case imdef:
        match ImageDef(TensorLike(dtype,arng,ch,vr),tags) if "en_batched" in tags and "B" in arng:
            return [Edge(
                a=imdef,
                b=ImageDef(imdef.data_type.__class__(dtype,arng[1:],ch,vr),tags-frozenset("en_batched")),
                f=a->a[0],
                cost=1,
                name=f"de_batch en_batched image"
            )]
        match ImageDef(PILImages(mode,ch),tags) if "en_batched" in tags:
            return [Edge(
                a=imdef,
                b=ImageDef(PILImage(mode,ch),tags-frozenset("en_batched")),
                f=a->a[0],
                cost=1,
                name=f"de_batch en_batched image"
            )]

def drop_batch_tag(imdef:ImageDef):
    case imdef:
        match ImageDef(data_type,tags) if "en_batched" in tags:
            return [Edge(imdef,
                         ImageDef(data_type,tags-frozenset(("en_batched",))),
                         f=a->a,
                         cost=1,
                         name="drop en_batched tag"
                        )]

@to_imagedef
def to_rgba(imdef:ImageDef):
    case imdef:
        match TensorLike(dtype,arng,ch_repr,vr) if len(ch_repr) == 4:
            return [Edge(a=imdef,
                         b=imdef.__class__(dtype,arng,"RGBA",vr),
                         f=a->a,
                         cost=10,
                         name=f"view {ch_repr} as RGBA "
                        )]
        match TensorLike(dtype,arng,ch_repr,vr) if len(ch_repr) == 3:
            return [Edge(a=imdef,
                         b=imdef.__class__(dtype,arng,"RGB",vr),
                         f=a->a,
                         cost=10,
                         name=f"view {ch_repr} as RGB "
                        )]
@to_imagedef
def change_value_range(imdef:ImageDef):
    case imdef:
        match TensorLike("float32" or "float64",arng,ch_repr,=VR_0_255):
            return [Edge(a=imdef,
                         b=imdef.__class__(imdef.dtype,arng,ch_repr,VR_0_1),
                         f=a->a/255.0,
                         cost=len(ch_repr),
                         name="0-255 to 0-1"
                        )]
        match TensorLike("float32" or "float64",arng,ch_repr,=VR_0_1):
            return [Edge(a=imdef,
                         b=imdef.__class__(imdef.dtype,arng,ch_repr,VR_0_255),
                         f=a->a*255.0,
                         cost=len(ch_repr),
                         name="0-1 to 0-255"
                        )]
    return []

def xyza_to_rgba(xyza):
    xyz = xyza[:3]
    a = xyza[[3]]
    rgb = (xyz+1)/2
    return np.concatenate((rgb,a),axis=0)
def xyz_to_rgb(xyz):
    return (xyz+1)/2
def rgb_to_xyz(rgb):
    return (rgb*2)-1
def rgba_to_xyza(rgba):
    rgb = rgba[:3]
    a = rgba[[3]]
    xyz = (rgb*2)-1
    return np.concatenate((xyz,a),axis=0)

def rule_xyz_to_rgb(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,"CHW","XYZA","-1_1"),tags):
            return [
                (xyza_to_rgba,ImageDef(Numpy(dtype,"CHW","RGBA",VR_0_1),tags),2,"xyza_to_rgba")
            ]
        match ImageDef(Numpy(dtype,"CHW","XYZ","-1_1"),tags):
            return [
                (xyz_to_rgb,ImageDef(Numpy(dtype,"CHW","RGB",VR_0_1),tags),2,"xyz_to_rgb")
            ]
        match ImageDef(Numpy(dtype,"CHW","RGBA",=VR_0_1),tags):
            return [
                (rgba_to_xyza,ImageDef(Numpy(dtype,"CHW","XYZA","-1_1"),tags),2,"rgba_to_xyza")
            ]
        match ImageDef(Numpy(dtype,"CHW","RGB",=VR_0_1),tags):
            return [
                (rgb_to_xyz,ImageDef(Numpy(dtype,"CHW","XYZ","-1_1"),tags),2,"rgb_to_xyz")
            ]

def b_xyza_to_rgba(xyza):
    xyz = xyza[:,:3]
    a = xyza[:,[3]]
    rgb = (xyz+1)/2
    return np.concatenate((rgb,a),axis=1)
def b_xyz_to_rgb(xyz):
    return (xyz+1)/2
def b_rgb_to_xyz(rgb):
    return (rgb*2)-1
def b_rgba_to_xyza(rgba):
    rgb = rgba[:,:3]
    a = rgba[:,[3]]
    xyz = (rgb*2)-1
    return np.concatenate((xyz,a),axis=1)

def rule_batch_xyz_to_rgb(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,"BCHW","XYZA","-1_1"),tags):
            return [
                (b_xyza_to_rgba,ImageDef(Numpy(dtype,"BCHW","RGBA",VR_0_1),tags),2,"xyza_to_rgba(batch)")
            ]
        match ImageDef(Numpy(dtype,"BCHW","XYZ","-1_1"),tags):
            return [
                (b_xyz_to_rgb,ImageDef(Numpy(dtype,"BCHW","RGB",VR_0_1),tags),2,"xyz_to_rgb(batch)")
            ]
        match ImageDef(Numpy(dtype,"BCHW","RGBA",=VR_0_1),tags):
            return [
                (b_rgba_to_xyza,ImageDef(Numpy(dtype,"BCHW","XYZA","-1_1"),tags),2,"rgba_to_xyza(batch)")
            ]
        match ImageDef(Numpy(dtype,"BCHW","RGB",=VR_0_1),tags):
            return [
                (b_rgb_to_xyz,ImageDef(Numpy(dtype,"BCHW","XYZ","-1_1"),tags),2,"rgb_to_xyz(batch)")
            ]



_conversions =[
    to_PILImages,
    to_numpy,
    to_torch,
    change_dtype,
    change_arrange,
    select_channel,
    drop_channel,
    en_batch,
    change_value_range,
    drop_alpha,
    to_rgba,
    drop_batch_tag,
    de_batch,
]


@memoize(1024)
def _edges(imdef):
    res = []
    for f in _conversions:
        edges = f(imdef)
        if edges is not None:
            res += edges
    return res




@memoize(1024)
def str_to_img_def(query):
    """
    ex1: 'numpy,float32,BCHW,RGB,0_255 | hello,world'
    ex2: 'torch,float32,BCHW,RGBA,0_1'
    ex3: 'image,RGBA,RGBA'
    ex4: 'images,RGB,RGB|tag1,tag2...'
    """
    vrs = {
        "0_255":VR_0_255,
        "0_1":VR_0_1,
        "None":VR_None
    }
    query = query.replace(" ","")
    def query_to_data_type(query):
        case query.split(","):
            match ["numpy",dtype,arng,ch,vr]:
                return Numpy(dtype,arng,ch,vrs[vr] if vr in vrs else vr)
            match ["torch",dtype,arng,ch,vr]:
                return Torch(dtype,arng,ch,vrs[vr] if vr in vrs else vr)
            match ["image",mode,ch]:
                return PILImage(mode,ch)
            match ["images",mode,ch]:
                return PILImages(mode,ch)
    case query.split("|"):
        match [data_type]:
            return ImageDef(query_to_data_type(data_type),frozenset())
        match [data_type,tags]:
            return ImageDef(query_to_data_type(data_type),frozenset(tags.split(",")))
    else:
        raise RuntimeError(f"could not parse image def string!:{query}")

def parse_def(img_def):
    try:
        return str_to_img_def(img_def) if img_def `isinstance` str else img_def
    except Exception as e:
        return img_def



accept_def_str = f -> parse_def ..> f
def imdef_neighbors(imdef):
    return [(e.f,e.b,e.cost,e.name) for e in _edges(imdef)]

#from data_tree.coconut.convert import AutoImage,PILImage,str_to_img_def,PILImages
#from data_tree.coconut.convert import ImageDef,Torch,Numpy,TensorLike,VR_0_1,VR_None,VR_0_255

def normalize_numpy_img(ary):
    _min = ary.min()
    _max = ary.max()
    return ((ary-_min)/(_max-_min))

def rule_VR_None_to_normalized(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,("CHW" or "HW") as arng,ch,=VR_None),tags):
            return [(
                normalize_numpy_img,
                ImageDef(Numpy(dtype,arng,ch,VR_0_1),tags),
                1,
                "minmax_0_1_numpy_img"
            )]
        match ImageDef(Numpy(dtype,"BCHW",ch,=VR_None),tags):
            return [(
                batch->np.array([normalize_numpy_img(img) for img in batch]),
                ImageDef(Numpy(dtype,"BCHW",ch,VR_0_1),tags),
                1,
                "batch_minmax_0_1_numpy_img"
            )]
def rule_add_channel(imdef):
    case imdef:
        match ImageDef(Numpy(dtype,"HW",ch,vr),tags):
            return [(
                a->a[None],
                ImageDef(Numpy(dtype,"CHW",ch,vr),tags),
                1,
                "add_channel_dim"
            )]
        match ImageDef(Numpy(dtype,"BHW",ch,vr),tags):
            return [(
                a->a[:,None],
                ImageDef(Numpy(dtype,"BCHW",ch,vr),tags),
                1,
                "add_channel_dim"
            )]
def rule_swap_RGB_BGR(imdef):
    case imdef:
        match ImageDef(TensorLike(dtype,"BHWC",("RGB" or "BGR") as rgb_order,vr) as tl,tags):
            return [(
                a->a[:,:,:,[2,1,0]],
                ImageDef(tl.__class__(dtype,"BHWC","RGB" if rgb_order.startswith("B") else "BGR",vr),tags),
                1,
                "swap rgb or bgr"
            )]
def rule_BGR_to_LAB(imdef):
    from skimage import color
    case imdef:
        match ImageDef(Numpy("float32","HWC","BGR",=VR_0_1),tags):
            return[(
                color.rgb2lab,
                ImageDef(Numpy("float32","HWC","LAB","VR_LAB"),tags),
                1,
                "bgr_0_1 to lab"
            )]
        match ImageDef(Numpy("float32","HWC","LAB","VR_LAB"),tags):
            return [(
                color.lab2rgb,
                ImageDef(Numpy("float32","HWC","BGR",VR_0_1),tags),
                1,
                "lab to bgr_0_1"
            )]




class AutoImage:
    default_rules = [
        imdef_neighbors,
        rule_xyz_to_rgb,
        rule_batch_xyz_to_rgb,
        rule_VR_None_to_normalized,
        rule_add_channel,
        rule_swap_RGB_BGR,
        rule_BGR_to_LAB
        ]
    solver = AStarSolver(rules=default_rules.copy())

    @staticmethod
    def reset_solver():
        AutoImage.solver = AStarSolver(rules = AutoImage.default_rules.copy())

    @staticmethod
    def debug_conversion(a,b,samples):
        x = samples
        edges = AutoImage.solver.search_direct(a,b).edges
        for edge in edges:
            print(edge)
            print(edge.f)
            x = edge.f(x)
            print(f"converted to type:{type(x)}")
            if x `isinstance` np.ndarray:
                print(x.shape)
            print(f"converted:{x}")
        return x

    def to_debug(self,img_def):
        img_def = parse_def(img_def)
        return AutoImage.debug_conversion(self.img_def,img_def,self.data)

    def __init__(self,data,img_def):
        img_def = parse_def(img_def)
        self.data = data
        self.img_def = img_def

    def converter(self,img_def):
        img_def = parse_def(img_def)
        return AutoImage.solver.search_direct(self.img_def,img_def)

    def any_converter(self,img_defs):
        imdefs = [parse_def(imdef) for imdef in img_defs]
        return AutoImage.solver.search_direct_any(self.img_def,imdefs)

    def convert(self,img_def):
        convert = self.converter(img_def)
        if convert.edges:
            return AutoImage(convert(self.data),convert.edges[-1].dst)
        else:
            return self

    def any_convert(self,imdefs):
        converter = self.any_converter(imdefs)
        if converter.edges:
            return AutoImage(converter(self.data),converter.edges[-1].dst)
        else:
            return self

    def to(self,img_def is (str,ImageDef),log_trace=False):
        return self.convert(img_def).data

    def any_to(self,imdefs):
        return self.any_convert(imdefs).data

    def to_widget(self):
        case self.img_def.data_type:
            match item is PILImages:
                return self.tile_image().to_widget()
            match TensorLike(_,arng,*_) if "B" in arng:
                return self.tile_image().to_widget()
        else:
            convert = self.converter(self.to_images_def())
        return convert(self.data) |> infer_widget

    def _repr_html_(self):
        return self.to_widget() |> display

    def to_images_def(self):
        """
        you have to add en_batched tag when data is not batch.
        """
        tag_opt=frozenset()
        img_cls = PILImages
        case self.img_def:
            match ImageDef(TensorLike(_,arng,_,_),tags) if "B" not in arng:
                tag_opt = frozenset(("en_batched",))
            match ImageDef(PILImage(mode,ch),tags):
                tag_opt = frozenset(("en_batched",))

        case self.img_def.data_type:
            match PILImage(mode,chrepr):
                return ImageDef(PILImages(mode,chrepr),self.img_def.tags | tag_opt)
            match PILImages(mode,chrepr):
                return self.img_def
            match TensorLike(dtype,arng,c,vr) if len(c) == 1:
                return ImageDef(img_cls("L",c),self.img_def.tags | tag_opt)
            match TensorLike(dtype,arng,"RGBA",vr):
                return ImageDef(img_cls("RGBA","RGBA"),self.img_def.tags | tag_opt)
            match TensorLike(dtype,arng,"RGB",vr):
                return ImageDef(img_cls("RGB","RGB"),self.img_def.tags | tag_opt)
            match TensorLike(dtype,arng,ch,vr) if "A" in ch and ch != "LAB":
                return ImageDef(img_cls("RGBA","RGBA"),self.img_def.tags | tag_opt)
        else:
            attempts=[
                "image,RGBA,RGBA",
                "images,RGBA,RGBA",
                "image,RGB,RGB",
                "images,RGB,RGB",
                "image,L,L",
                "images,L,L"
            ]
            for tgt in attempts:
                imdef = str_to_img_def(tgt)
                try:
                    converter = self.converter(imdef)
                    return imdef
                except NoRouteException as e:
                    #logger.warning(f"no route for:{imdef}. trying next imdef.")
                    pass
            raise RuntimeError(f"cannot convert to image:{self.img_def} to any image_like imdef}")


    def image_op(self,f:Image->Image):
        images_def = self.to_images_def()
        images = self.to(images_def)
        new_images=[f(i) for i in images ] # do some resizing or something
        new_ai = AutoImage(new_images,images_def)
        return new_ai.convert(self.img_def) # go back to last state.

    def visdom(self,visdom=None,**kwargs):
        if visdom is None:
            from data_tree.visdom import VISDOM
            visdom = VISDOM
        candidates = [
           "numpy,float32,CHW,RGB,0_1",
           "numpy,float32,CHW,L,0_1",
           "numpy,float32,BCHW,RGB,0_1",
           "numpy,float32,BCHW,L,0_1"
        ]
        img = self.any_convert(candidates)
        data_type = img.img_def.data_type
        case data_type:
           match Numpy(_,"CHW",_,_):
               res = visdom.image(img.data,**kwargs)
           match Numpy(_,"BCHW",_,_):
               res = visdom.images(img.data,**kwargs)
        return res


@property
def AutoImage.images(self):
    return self.to(self.to_images_def())

@property
def AutoImage.image_size(self):
    return self.images[0].size

def make_grid(imgs, nrow, padding=0):
    """Numpy配列の複数枚の画像を、1枚の画像にタイルします

    Arguments:
        imgs {np.ndarray} -- 複数枚の画像からなるテンソル
        nrow {int} -- 1行あたりにタイルする枚数

    Keyword Arguments:
        padding {int} -- グリッドの間隔 (default: {0})

    Returns:
        [np.ndarray] -- 3階テンソル。1枚の画像
    """
    assert imgs.ndim == 4 and nrow > 0
    batch, height, width, ch = imgs.shape
    n = nrow * (batch // nrow + np.sign(batch % nrow))
    ncol = n // nrow
    pad = np.zeros((n - batch, height, width, ch), imgs.dtype)
    x = np.concatenate([imgs, pad], axis=0)
    # border padding if required
    if padding > 0:
        x = np.pad(x, ((0, 0), (0, padding), (0, padding), (0, 0)),
                   "constant", constant_values=(0, 0)) # 下と右だけにpaddingを入れる
        height += padding
        width += padding
    x = x.reshape(ncol, nrow, height, width, ch)
    x = x.transpose([0, 2, 1, 3, 4])  # (ncol, height, nrow, width, ch)
    x = x.reshape(height * ncol, width * nrow, ch)
    if padding > 0:
        x = x[:(height * ncol - padding),:(width * nrow - padding),:] # 右端と下端のpaddingを削除
    return x


def AutoImage.tile_image(self,w=1024,h=1024,max_image=100,padding=1):
    ch = self.to_images_def().data_type.channel_repr
    if len(ch) == 1:
        codec = f"numpy,uint8,BHW,{ch},0_255"
    else:
        codec = f"numpy,uint8,BHWC,{ch},0_255"
    imgs = self.to(codec)[:max_image]
    nrow = int(sqrt(max_image)+0.5)
    r = int((w-((nrow+1)*padding))/nrow)
    imgs = np.array([(Image.fromarray(img).resize((r,r)) |> np.array) for img in imgs])
    if len(ch) == 1:
        imgs = imgs[:,:,:,None]
    return AutoImage(make_grid(imgs,nrow,padding=1),f"numpy,uint8,HWC,{ch},0_255")

img_to_shifting_grids = img->make_grids(*img.image_size)|> shifting_grids
def auto_to_3res(img:"AutoImage",cx,cy,r=256)->"AutoImage":
    img = img.to("image,L,L")
    #img = img.resize((2048,2048))
    chs = [crop_square(img,cx,cy,_r).resize((r,r)) for _r in [r*4,r*2,r]]
    return AutoImage(np.concatenate([np.array(i)[:,:,None] for i in chs],axis=2),"numpy,float32,HWC,RGB,0_255")

def img_to_grid_batch(img:AutoImage):
    grids = img_to_shifting_grids(img) |> .astype("int32") |> series
    batch = grids.map((xy)->auto_to_3res(img,xy[0]+128,xy[1]+128,r=256).to("numpy,float32,HWC,RGB,0_1")).values |> array |> .astype("float32")
    return grids.values,AutoImage(batch,"numpy,float32,BHWC,RGB,0_1")


def AutoImage.cast(self,imgdef):
    return AutoImage(self.data,imgdef)


